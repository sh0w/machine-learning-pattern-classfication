{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contains Neural Network and IID Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate Baseline Score!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "#just for testing:\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dataframe\n",
    "df = pd.read_csv(\"dataset/phase_3_TRAIN_7d499bff69ca69b6_6372c3e_MLPC2021_generic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop target value, student annotations and string ID from input features:\n",
    "X = df.drop(columns=['quadrant','mean_A','mean_V','id','score_mode','score_key_strength'])\n",
    "\n",
    "# we want to predict the quadrant:\n",
    "y = df['quadrant'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Pianist</th>\n",
       "      <th>Piece_id</th>\n",
       "      <th>Snippet_number</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GG-01-000</td>\n",
       "      <td>GG</td>\n",
       "      <td>01</td>\n",
       "      <td>000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GG-01-001</td>\n",
       "      <td>GG</td>\n",
       "      <td>01</td>\n",
       "      <td>001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GG-01-002</td>\n",
       "      <td>GG</td>\n",
       "      <td>01</td>\n",
       "      <td>002</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GG-01-003</td>\n",
       "      <td>GG</td>\n",
       "      <td>01</td>\n",
       "      <td>003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GG-01-004</td>\n",
       "      <td>GG</td>\n",
       "      <td>01</td>\n",
       "      <td>004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id Pianist Piece_id Snippet_number  class\n",
       "0  GG-01-000      GG       01            000      1\n",
       "1  GG-01-001      GG       01            001      1\n",
       "2  GG-01-002      GG       01            002      1\n",
       "3  GG-01-003      GG       01            003      1\n",
       "4  GG-01-004      GG       01            004      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting by pianist and piecw!\n",
    "\n",
    "#create tags_dataframe:\n",
    "X_tags=pd.DataFrame()\n",
    "X_tags['id']=df['id']\n",
    "\n",
    "\n",
    "#extract piece_id and pianist to later allow by piece/pianist/both cross validation\n",
    "def extractPianist(x):\n",
    "    return x[0:2]\n",
    "def extract_piece_id(x):\n",
    "    return x[3:5]\n",
    "def extract_snippet_number(x):\n",
    "    return x[6:9]\n",
    "\n",
    "X_tags['Pianist']=X_tags['id'].apply(extractPianist)\n",
    "X_tags['Piece_id']=X_tags['id'].apply(extract_piece_id)\n",
    "X_tags['Snippet_number']=X_tags['id'].apply(extract_snippet_number)\n",
    "X_tags['class']=df['quadrant']\n",
    "\n",
    "X_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get list of pianists and pieces!\n",
    "pianist_list=list(set(X_tags['Pianist']))\n",
    "piece_list=list(set(X_tags['Piece_id']))\n",
    "len(piece_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get distribution of classes per pianist\n",
    "pianist_dist_list=[]\n",
    "pianist_dist_list_perecent=[]\n",
    "for pianist in pianist_list:  \n",
    "    mylist=[]\n",
    "    for i in range(4):\n",
    "        mylist.append(len(X_tags.loc[(X_tags['Pianist'] == pianist) & (X_tags['class']==i+1)]))\n",
    "    pianist_dist_list.append(mylist)\n",
    "    mylist=[element/sum(mylist) for element in mylist]\n",
    "    pianist_dist_list_perecent.append(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[70, 46, 294, 72],\n",
       " [73, 48, 210, 95],\n",
       " [95, 62, 148, 65],\n",
       " [71, 58, 183, 70],\n",
       " [70, 28, 280, 83],\n",
       " [94, 32, 265, 95]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pianist_dist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution of classes per pianist seems to be fairly uniform accross all pianist: ->reandom cv_split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose random pianists\n",
    "pianist_cv=[]\n",
    "number_folds=3\n",
    "\n",
    "#initialize pianists_cv:\n",
    "for i in range(number_folds):\n",
    "    pianist_cv.append([])\n",
    "    \n",
    "#assign pianists to folds randomly\n",
    "cp_pianist_list=pianist_list.copy()\n",
    "for i in range(len(cp_pianist_list)):\n",
    "    choosen_pianist=cp_pianist_list[np.random.randint(len(cp_pianist_list), size=1)[0]]\n",
    "    cp_pianist_list.remove(choosen_pianist)\n",
    "    pianist_cv[len(cp_pianist_list)%number_folds].append(choosen_pianist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GG', 'FG'], ['RT', 'AS'], ['SR', 'AH']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pianist_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get distribution of classes per piece\n",
    "piece_dist_list=[]\n",
    "piece_dist_list_perecent=[]\n",
    "for piece in piece_list:  \n",
    "    mylist=[]\n",
    "    for i in range(4):\n",
    "        mylist.append(len(X_tags.loc[(X_tags['Piece_id'] == piece) & (X_tags['class']==i+1)]))\n",
    "    piece_dist_list.append(mylist)\n",
    "    mylist=[element/sum(mylist) for element in mylist]\n",
    "    piece_dist_list_perecent.append(mylist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#piece_dist_list \n",
    "# shows that a lot of pieces are only labled as one emotion! To ensure that all 4 emotions a present\n",
    "# in the test set we can not do a completly random split! IDEA: Create 4 Subesets such that in each subset the majority class of \n",
    "# every piece is the same and then assign sumples from each subset randomly and uiformly to the train test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 4 subsets:\n",
    "piece_subsets=[]\n",
    "\n",
    "#initialize piece_cv:\n",
    "for i in range(4):\n",
    "    piece_subsets.append([])\n",
    "piece_list_cp=copy.deepcopy(piece_list)\n",
    "for p,piece in enumerate(piece_list):\n",
    "    done=False\n",
    "    for i in range(4):\n",
    "        if not done:\n",
    "            #print(piece_dist_list[p][i])\n",
    "            #print(max(piece_dist_list[p]))\n",
    "            if piece_dist_list[p][i]==max(piece_dist_list[p][:]):\n",
    "                piece_subsets[i].append(piece_list[p])\n",
    "                piece_list_cp.remove\n",
    "                done=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just double checking!\n",
    "np.shape(piece_subsets)\n",
    "piece_subsets\n",
    "mysum=0\n",
    "for i in range(4):\n",
    "    mysum+=len(piece_subsets[i])\n",
    "mysum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose random pieces using above subset technique:\n",
    "piece_cv=[]\n",
    "number_folds=5\n",
    "#initialize piece_cv:\n",
    "for i in range(number_folds):\n",
    "    piece_cv.append([])\n",
    "    \n",
    "cp_pi_subs = copy.deepcopy(piece_subsets)\n",
    "#assign pieces to folds randomly\n",
    "for i in range(4):\n",
    "    fold=0\n",
    "    for p in range(len(piece_subsets[i])):\n",
    "        chos_piece=cp_pi_subs[i][np.random.randint(len(cp_pi_subs[i]), size=1)[0]]\n",
    "        cp_pi_subs[i].remove(chos_piece)\n",
    "        piece_cv[fold].append(chos_piece)\n",
    "        if fold>=number_folds-1:\n",
    "            fold=0\n",
    "        else:\n",
    "            fold+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['27', '41', '39', '04', '15', '35', '16', '46', '45'],\n",
       " ['37', '29', '11', '47', '08', '02', '34'],\n",
       " ['30', '21', '20', '32', '28', '25', '26'],\n",
       " ['05', '18', '03', '12', '24', '01', '22'],\n",
       " ['09', '14', '31', '36', '19', '38']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piece_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just double checking!\n",
    "np.shape(piece_cv)\n",
    "piece_subsets\n",
    "mysum=0\n",
    "for i in range(len(piece_cv)):\n",
    "    mysum+=len(piece_cv[i])\n",
    "mysum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  mask=np.where((X_tags['Pianist'] == pianist_cv[f][i]))[0].index.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([8.520e+02, 8.530e+02, 8.540e+02, 8.550e+02, 8.560e+02, 8.570e+02,\n",
       "         8.580e+02, 8.590e+02, 8.600e+02, 8.610e+02, 8.620e+02, 8.630e+02,\n",
       "         8.640e+02, 8.650e+02, 8.660e+02, 8.670e+02, 8.680e+02, 8.690e+02,\n",
       "         8.700e+02, 8.710e+02, 8.720e+02, 8.730e+02, 8.740e+02, 8.750e+02,\n",
       "         8.760e+02, 8.770e+02, 8.780e+02, 8.790e+02, 8.800e+02, 8.810e+02,\n",
       "         8.820e+02, 8.830e+02, 8.840e+02, 8.850e+02, 8.860e+02, 8.870e+02,\n",
       "         8.880e+02, 8.890e+02, 8.900e+02, 8.910e+02, 8.920e+02, 8.930e+02,\n",
       "         8.940e+02, 8.950e+02, 8.960e+02, 8.970e+02, 8.980e+02, 8.990e+02,\n",
       "         9.000e+02, 9.010e+02, 9.020e+02, 9.030e+02, 9.040e+02, 9.050e+02,\n",
       "         9.060e+02, 9.070e+02, 9.080e+02, 9.090e+02, 9.100e+02, 9.110e+02,\n",
       "         9.120e+02, 9.130e+02, 9.140e+02, 9.150e+02, 9.160e+02, 9.170e+02,\n",
       "         9.180e+02, 9.190e+02, 9.200e+02, 9.210e+02, 9.220e+02, 9.230e+02,\n",
       "         9.240e+02, 9.250e+02, 9.260e+02, 9.270e+02, 9.280e+02, 9.290e+02,\n",
       "         9.300e+02, 9.310e+02, 9.320e+02, 9.330e+02, 9.340e+02, 9.350e+02,\n",
       "         9.360e+02, 9.370e+02, 9.380e+02, 9.390e+02, 9.400e+02, 9.410e+02,\n",
       "         9.420e+02, 9.430e+02, 9.440e+02, 9.450e+02, 9.460e+02, 9.470e+02,\n",
       "         9.480e+02, 9.490e+02, 9.500e+02, 9.510e+02, 9.520e+02, 9.530e+02,\n",
       "         9.540e+02, 9.550e+02, 9.560e+02, 9.570e+02, 9.580e+02, 9.590e+02,\n",
       "         9.600e+02, 9.610e+02, 9.620e+02, 9.630e+02, 9.640e+02, 9.650e+02,\n",
       "         9.660e+02, 9.670e+02, 9.680e+02, 9.690e+02, 9.700e+02, 9.710e+02,\n",
       "         9.720e+02, 9.730e+02, 9.740e+02, 9.750e+02, 9.760e+02, 9.770e+02,\n",
       "         9.780e+02, 9.790e+02, 9.800e+02, 9.810e+02, 9.820e+02, 9.830e+02,\n",
       "         9.840e+02, 9.850e+02, 9.860e+02, 9.870e+02, 9.880e+02, 9.890e+02,\n",
       "         9.900e+02, 9.910e+02, 9.920e+02, 9.930e+02, 9.940e+02, 9.950e+02,\n",
       "         9.960e+02, 9.970e+02, 9.980e+02, 9.990e+02, 1.000e+03, 1.001e+03,\n",
       "         1.002e+03, 1.003e+03, 1.004e+03, 1.005e+03, 1.006e+03, 1.007e+03,\n",
       "         1.008e+03, 1.009e+03, 1.010e+03, 1.011e+03, 1.012e+03, 1.013e+03,\n",
       "         1.014e+03, 1.015e+03, 1.016e+03, 1.017e+03, 1.018e+03, 1.019e+03,\n",
       "         1.020e+03, 1.021e+03, 1.022e+03, 1.023e+03, 1.024e+03, 1.025e+03,\n",
       "         1.026e+03, 1.027e+03, 1.028e+03, 1.029e+03, 1.030e+03, 1.031e+03,\n",
       "         1.032e+03, 1.033e+03, 1.034e+03, 1.035e+03, 1.036e+03, 1.037e+03,\n",
       "         1.038e+03, 1.039e+03, 1.040e+03, 1.041e+03, 1.042e+03, 1.043e+03,\n",
       "         1.044e+03, 1.045e+03, 1.046e+03, 1.047e+03, 1.048e+03, 1.049e+03,\n",
       "         1.050e+03, 1.051e+03, 1.052e+03, 1.053e+03, 1.054e+03, 1.055e+03,\n",
       "         1.056e+03, 1.057e+03, 1.058e+03, 1.059e+03, 1.060e+03, 1.061e+03,\n",
       "         1.062e+03, 1.063e+03, 1.064e+03, 1.065e+03, 1.066e+03, 1.067e+03,\n",
       "         1.068e+03, 1.069e+03, 1.070e+03, 1.071e+03, 1.072e+03, 1.073e+03,\n",
       "         1.074e+03, 1.075e+03, 1.076e+03, 1.077e+03, 1.078e+03, 1.079e+03,\n",
       "         1.080e+03, 1.081e+03, 1.082e+03, 1.083e+03, 1.084e+03, 1.085e+03,\n",
       "         1.086e+03, 1.087e+03, 1.088e+03, 1.089e+03, 1.090e+03, 1.091e+03,\n",
       "         1.092e+03, 1.093e+03, 1.094e+03, 1.095e+03, 1.096e+03, 1.097e+03,\n",
       "         1.098e+03, 1.099e+03, 1.100e+03, 1.101e+03, 1.102e+03, 1.103e+03,\n",
       "         1.104e+03, 1.105e+03, 1.106e+03, 1.107e+03, 1.108e+03, 1.109e+03,\n",
       "         1.110e+03, 1.111e+03, 1.112e+03, 1.113e+03, 1.114e+03, 1.115e+03,\n",
       "         1.116e+03, 1.117e+03, 1.118e+03, 1.119e+03, 1.120e+03, 1.121e+03,\n",
       "         1.122e+03, 1.123e+03, 1.124e+03, 1.125e+03, 1.126e+03, 1.127e+03,\n",
       "         1.128e+03, 1.129e+03, 1.130e+03, 1.131e+03, 1.132e+03, 1.133e+03,\n",
       "         1.134e+03, 1.135e+03, 1.136e+03, 1.137e+03, 1.138e+03, 1.139e+03,\n",
       "         1.140e+03, 1.141e+03, 1.142e+03, 1.143e+03, 1.144e+03, 1.145e+03,\n",
       "         1.146e+03, 1.147e+03, 1.148e+03, 1.149e+03, 1.150e+03, 1.151e+03,\n",
       "         1.152e+03, 1.153e+03, 1.154e+03, 1.155e+03, 1.156e+03, 1.157e+03,\n",
       "         1.158e+03, 1.159e+03, 1.160e+03, 1.161e+03, 1.162e+03, 1.163e+03,\n",
       "         1.164e+03, 1.165e+03, 1.166e+03, 1.167e+03, 1.168e+03, 1.169e+03,\n",
       "         1.170e+03, 1.171e+03, 1.172e+03, 1.173e+03, 1.174e+03, 1.175e+03,\n",
       "         1.176e+03, 1.177e+03, 1.178e+03, 1.179e+03, 1.180e+03, 1.181e+03,\n",
       "         1.182e+03, 1.183e+03, 1.184e+03, 1.185e+03, 1.186e+03, 1.187e+03,\n",
       "         1.188e+03, 1.189e+03, 1.190e+03, 1.191e+03, 1.192e+03, 1.193e+03,\n",
       "         1.194e+03, 1.195e+03, 1.196e+03, 1.197e+03, 1.198e+03, 1.199e+03,\n",
       "         1.200e+03, 1.201e+03, 1.202e+03, 1.203e+03, 1.204e+03, 1.205e+03,\n",
       "         1.206e+03, 1.207e+03, 1.208e+03, 1.209e+03, 1.210e+03, 1.211e+03,\n",
       "         1.212e+03, 1.213e+03, 1.214e+03, 1.215e+03, 1.216e+03, 1.217e+03,\n",
       "         1.218e+03, 1.219e+03, 1.220e+03, 1.221e+03, 1.222e+03, 1.223e+03,\n",
       "         1.224e+03, 1.225e+03, 1.226e+03, 1.227e+03, 1.228e+03, 1.229e+03,\n",
       "         1.230e+03, 1.231e+03, 1.232e+03, 1.233e+03, 1.234e+03, 1.235e+03,\n",
       "         1.236e+03, 1.237e+03, 1.238e+03, 1.239e+03, 1.240e+03, 1.241e+03,\n",
       "         1.242e+03, 1.243e+03, 1.244e+03, 1.245e+03, 1.246e+03, 1.247e+03,\n",
       "         1.248e+03, 1.249e+03, 1.250e+03, 1.251e+03, 1.252e+03, 1.253e+03,\n",
       "         1.254e+03, 1.255e+03, 1.256e+03, 1.257e+03, 1.258e+03, 1.259e+03,\n",
       "         1.260e+03, 1.261e+03, 1.262e+03, 1.263e+03, 1.264e+03, 1.265e+03,\n",
       "         1.266e+03, 1.267e+03, 1.268e+03, 1.269e+03, 1.270e+03, 1.271e+03,\n",
       "         1.272e+03, 1.273e+03, 1.274e+03, 1.275e+03, 1.276e+03, 1.277e+03,\n",
       "         0.000e+00, 1.000e+00, 2.000e+00, 3.000e+00, 4.000e+00, 5.000e+00,\n",
       "         6.000e+00, 7.000e+00, 8.000e+00, 9.000e+00, 1.000e+01, 1.100e+01,\n",
       "         1.200e+01, 1.300e+01, 1.400e+01, 1.500e+01, 1.600e+01, 1.700e+01,\n",
       "         1.800e+01, 1.900e+01, 2.000e+01, 2.100e+01, 2.200e+01, 2.300e+01,\n",
       "         2.400e+01, 2.500e+01, 2.600e+01, 2.700e+01, 2.800e+01, 2.900e+01,\n",
       "         3.000e+01, 3.100e+01, 3.200e+01, 3.300e+01, 3.400e+01, 3.500e+01,\n",
       "         3.600e+01, 3.700e+01, 3.800e+01, 3.900e+01, 4.000e+01, 4.100e+01,\n",
       "         4.200e+01, 4.300e+01, 4.400e+01, 4.500e+01, 4.600e+01, 4.700e+01,\n",
       "         4.800e+01, 4.900e+01, 5.000e+01, 5.100e+01, 5.200e+01, 5.300e+01,\n",
       "         5.400e+01, 5.500e+01, 5.600e+01, 5.700e+01, 5.800e+01, 5.900e+01,\n",
       "         6.000e+01, 6.100e+01, 6.200e+01, 6.300e+01, 6.400e+01, 6.500e+01,\n",
       "         6.600e+01, 6.700e+01, 6.800e+01, 6.900e+01, 7.000e+01, 7.100e+01,\n",
       "         7.200e+01, 7.300e+01, 7.400e+01, 7.500e+01, 7.600e+01, 7.700e+01,\n",
       "         7.800e+01, 7.900e+01, 8.000e+01, 8.100e+01, 8.200e+01, 8.300e+01,\n",
       "         8.400e+01, 8.500e+01, 8.600e+01, 8.700e+01, 8.800e+01, 8.900e+01,\n",
       "         9.000e+01, 9.100e+01, 9.200e+01, 9.300e+01, 9.400e+01, 9.500e+01,\n",
       "         9.600e+01, 9.700e+01, 9.800e+01, 9.900e+01, 1.000e+02, 1.010e+02,\n",
       "         1.020e+02, 1.030e+02, 1.040e+02, 1.050e+02, 1.060e+02, 1.070e+02,\n",
       "         1.080e+02, 1.090e+02, 1.100e+02, 1.110e+02, 1.120e+02, 1.130e+02,\n",
       "         1.140e+02, 1.150e+02, 1.160e+02, 1.170e+02, 1.180e+02, 1.190e+02,\n",
       "         1.200e+02, 1.210e+02, 1.220e+02, 1.230e+02, 1.240e+02, 1.250e+02,\n",
       "         1.260e+02, 1.270e+02, 1.280e+02, 1.290e+02, 1.300e+02, 1.310e+02,\n",
       "         1.320e+02, 1.330e+02, 1.340e+02, 1.350e+02, 1.360e+02, 1.370e+02,\n",
       "         1.380e+02, 1.390e+02, 1.400e+02, 1.410e+02, 1.420e+02, 1.430e+02,\n",
       "         1.440e+02, 1.450e+02, 1.460e+02, 1.470e+02, 1.480e+02, 1.490e+02,\n",
       "         1.500e+02, 1.510e+02, 1.520e+02, 1.530e+02, 1.540e+02, 1.550e+02,\n",
       "         1.560e+02, 1.570e+02, 1.580e+02, 1.590e+02, 1.600e+02, 1.610e+02,\n",
       "         1.620e+02, 1.630e+02, 1.640e+02, 1.650e+02, 1.660e+02, 1.670e+02,\n",
       "         1.680e+02, 1.690e+02, 1.700e+02, 1.710e+02, 1.720e+02, 1.730e+02,\n",
       "         1.740e+02, 1.750e+02, 1.760e+02, 1.770e+02, 1.780e+02, 1.790e+02,\n",
       "         1.800e+02, 1.810e+02, 1.820e+02, 1.830e+02, 1.840e+02, 1.850e+02,\n",
       "         1.860e+02, 1.870e+02, 1.880e+02, 1.890e+02, 1.900e+02, 1.910e+02,\n",
       "         1.920e+02, 1.930e+02, 1.940e+02, 1.950e+02, 1.960e+02, 1.970e+02,\n",
       "         1.980e+02, 1.990e+02, 2.000e+02, 2.010e+02, 2.020e+02, 2.030e+02,\n",
       "         2.040e+02, 2.050e+02, 2.060e+02, 2.070e+02, 2.080e+02, 2.090e+02,\n",
       "         2.100e+02, 2.110e+02, 2.120e+02, 2.130e+02, 2.140e+02, 2.150e+02,\n",
       "         2.160e+02, 2.170e+02, 2.180e+02, 2.190e+02, 2.200e+02, 2.210e+02,\n",
       "         2.220e+02, 2.230e+02, 2.240e+02, 2.250e+02, 2.260e+02, 2.270e+02,\n",
       "         2.280e+02, 2.290e+02, 2.300e+02, 2.310e+02, 2.320e+02, 2.330e+02,\n",
       "         2.340e+02, 2.350e+02, 2.360e+02, 2.370e+02, 2.380e+02, 2.390e+02,\n",
       "         2.400e+02, 2.410e+02, 2.420e+02, 2.430e+02, 2.440e+02, 2.450e+02,\n",
       "         2.460e+02, 2.470e+02, 2.480e+02, 2.490e+02, 2.500e+02, 2.510e+02,\n",
       "         2.520e+02, 2.530e+02, 2.540e+02, 2.550e+02, 2.560e+02, 2.570e+02,\n",
       "         2.580e+02, 2.590e+02, 2.600e+02, 2.610e+02, 2.620e+02, 2.630e+02,\n",
       "         2.640e+02, 2.650e+02, 2.660e+02, 2.670e+02, 2.680e+02, 2.690e+02,\n",
       "         2.700e+02, 2.710e+02, 2.720e+02, 2.730e+02, 2.740e+02, 2.750e+02,\n",
       "         2.760e+02, 2.770e+02, 2.780e+02, 2.790e+02, 2.800e+02, 2.810e+02,\n",
       "         2.820e+02, 2.830e+02, 2.840e+02, 2.850e+02, 2.860e+02, 2.870e+02,\n",
       "         2.880e+02, 2.890e+02, 2.900e+02, 2.910e+02, 2.920e+02, 2.930e+02,\n",
       "         2.940e+02, 2.950e+02, 2.960e+02, 2.970e+02, 2.980e+02, 2.990e+02,\n",
       "         3.000e+02, 3.010e+02, 3.020e+02, 3.030e+02, 3.040e+02, 3.050e+02,\n",
       "         3.060e+02, 3.070e+02, 3.080e+02, 3.090e+02, 3.100e+02, 3.110e+02,\n",
       "         3.120e+02, 3.130e+02, 3.140e+02, 3.150e+02, 3.160e+02, 3.170e+02,\n",
       "         3.180e+02, 3.190e+02, 3.200e+02, 3.210e+02, 3.220e+02, 3.230e+02,\n",
       "         3.240e+02, 3.250e+02, 3.260e+02, 3.270e+02, 3.280e+02, 3.290e+02,\n",
       "         3.300e+02, 3.310e+02, 3.320e+02, 3.330e+02, 3.340e+02, 3.350e+02,\n",
       "         3.360e+02, 3.370e+02, 3.380e+02, 3.390e+02, 3.400e+02, 3.410e+02,\n",
       "         3.420e+02, 3.430e+02, 3.440e+02, 3.450e+02, 3.460e+02, 3.470e+02,\n",
       "         3.480e+02, 3.490e+02, 3.500e+02, 3.510e+02, 3.520e+02, 3.530e+02,\n",
       "         3.540e+02, 3.550e+02, 3.560e+02, 3.570e+02, 3.580e+02, 3.590e+02,\n",
       "         3.600e+02, 3.610e+02, 3.620e+02, 3.630e+02, 3.640e+02, 3.650e+02,\n",
       "         3.660e+02, 3.670e+02, 3.680e+02, 3.690e+02]),\n",
       "  array([370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382,\n",
       "         383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395,\n",
       "         396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408,\n",
       "         409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421,\n",
       "         422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434,\n",
       "         435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
       "         448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460,\n",
       "         461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473,\n",
       "         474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
       "         487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
       "         500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
       "         513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
       "         526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
       "         539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
       "         552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564,\n",
       "         565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577,\n",
       "         578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590,\n",
       "         591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603,\n",
       "         604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616,\n",
       "         617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629,\n",
       "         630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642,\n",
       "         643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655,\n",
       "         656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668,\n",
       "         669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681,\n",
       "         682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694,\n",
       "         695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707,\n",
       "         708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720,\n",
       "         721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733,\n",
       "         734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
       "         747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759,\n",
       "         760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772,\n",
       "         773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785,\n",
       "         786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798,\n",
       "         799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811,\n",
       "         812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824,\n",
       "         825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837,\n",
       "         838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850,\n",
       "         851])),\n",
       " (array([ 370.,  371.,  372.,  373.,  374.,  375.,  376.,  377.,  378.,\n",
       "          379.,  380.,  381.,  382.,  383.,  384.,  385.,  386.,  387.,\n",
       "          388.,  389.,  390.,  391.,  392.,  393.,  394.,  395.,  396.,\n",
       "          397.,  398.,  399.,  400.,  401.,  402.,  403.,  404.,  405.,\n",
       "          406.,  407.,  408.,  409.,  410.,  411.,  412.,  413.,  414.,\n",
       "          415.,  416.,  417.,  418.,  419.,  420.,  421.,  422.,  423.,\n",
       "          424.,  425.,  426.,  427.,  428.,  429.,  430.,  431.,  432.,\n",
       "          433.,  434.,  435.,  436.,  437.,  438.,  439.,  440.,  441.,\n",
       "          442.,  443.,  444.,  445.,  446.,  447.,  448.,  449.,  450.,\n",
       "          451.,  452.,  453.,  454.,  455.,  456.,  457.,  458.,  459.,\n",
       "          460.,  461.,  462.,  463.,  464.,  465.,  466.,  467.,  468.,\n",
       "          469.,  470.,  471.,  472.,  473.,  474.,  475.,  476.,  477.,\n",
       "          478.,  479.,  480.,  481.,  482.,  483.,  484.,  485.,  486.,\n",
       "          487.,  488.,  489.,  490.,  491.,  492.,  493.,  494.,  495.,\n",
       "          496.,  497.,  498.,  499.,  500.,  501.,  502.,  503.,  504.,\n",
       "          505.,  506.,  507.,  508.,  509.,  510.,  511.,  512.,  513.,\n",
       "          514.,  515.,  516.,  517.,  518.,  519.,  520.,  521.,  522.,\n",
       "          523.,  524.,  525.,  526.,  527.,  528.,  529.,  530.,  531.,\n",
       "          532.,  533.,  534.,  535.,  536.,  537.,  538.,  539.,  540.,\n",
       "          541.,  542.,  543.,  544.,  545.,  546.,  547.,  548.,  549.,\n",
       "          550.,  551.,  552.,  553.,  554.,  555.,  556.,  557.,  558.,\n",
       "          559.,  560.,  561.,  562.,  563.,  564.,  565.,  566.,  567.,\n",
       "          568.,  569.,  570.,  571.,  572.,  573.,  574.,  575.,  576.,\n",
       "          577.,  578.,  579.,  580.,  581.,  582.,  583.,  584.,  585.,\n",
       "          586.,  587.,  588.,  589.,  590.,  591.,  592.,  593.,  594.,\n",
       "          595.,  596.,  597.,  598.,  599.,  600.,  601.,  602.,  603.,\n",
       "          604.,  605.,  606.,  607.,  608.,  609.,  610.,  611.,  612.,\n",
       "          613.,  614.,  615.,  616.,  617.,  618.,  619.,  620.,  621.,\n",
       "          622.,  623.,  624.,  625.,  626.,  627.,  628.,  629.,  630.,\n",
       "          631.,  632.,  633.,  634.,  635.,  636.,  637.,  638.,  639.,\n",
       "          640.,  641.,  642.,  643.,  644.,  645.,  646.,  647.,  648.,\n",
       "          649.,  650.,  651.,  652.,  653.,  654.,  655.,  656.,  657.,\n",
       "          658.,  659.,  660.,  661.,  662.,  663.,  664.,  665.,  666.,\n",
       "          667.,  668.,  669.,  670.,  671.,  672.,  673.,  674.,  675.,\n",
       "          676.,  677.,  678.,  679.,  680.,  681.,  682.,  683.,  684.,\n",
       "          685.,  686.,  687.,  688.,  689.,  690.,  691.,  692.,  693.,\n",
       "          694.,  695.,  696.,  697.,  698.,  699.,  700.,  701.,  702.,\n",
       "          703.,  704.,  705.,  706.,  707.,  708.,  709.,  710.,  711.,\n",
       "          712.,  713.,  714.,  715.,  716.,  717.,  718.,  719.,  720.,\n",
       "          721.,  722.,  723.,  724.,  725.,  726.,  727.,  728.,  729.,\n",
       "          730.,  731.,  732.,  733.,  734.,  735.,  736.,  737.,  738.,\n",
       "          739.,  740.,  741.,  742.,  743.,  744.,  745.,  746.,  747.,\n",
       "          748.,  749.,  750.,  751.,  752.,  753.,  754.,  755.,  756.,\n",
       "          757.,  758.,  759.,  760.,  761.,  762.,  763.,  764.,  765.,\n",
       "          766.,  767.,  768.,  769.,  770.,  771.,  772.,  773.,  774.,\n",
       "          775.,  776.,  777.,  778.,  779.,  780.,  781.,  782.,  783.,\n",
       "          784.,  785.,  786.,  787.,  788.,  789.,  790.,  791.,  792.,\n",
       "          793.,  794.,  795.,  796.,  797.,  798.,  799.,  800.,  801.,\n",
       "          802.,  803.,  804.,  805.,  806.,  807.,  808.,  809.,  810.,\n",
       "          811.,  812.,  813.,  814.,  815.,  816.,  817.,  818.,  819.,\n",
       "          820.,  821.,  822.,  823.,  824.,  825.,  826.,  827.,  828.,\n",
       "          829.,  830.,  831.,  832.,  833.,  834.,  835.,  836.,  837.,\n",
       "          838.,  839.,  840.,  841.,  842.,  843.,  844.,  845.,  846.,\n",
       "          847.,  848.,  849.,  850.,  851., 2121., 2122., 2123., 2124.,\n",
       "         2125., 2126., 2127., 2128., 2129., 2130., 2131., 2132., 2133.,\n",
       "         2134., 2135., 2136., 2137., 2138., 2139., 2140., 2141., 2142.,\n",
       "         2143., 2144., 2145., 2146., 2147., 2148., 2149., 2150., 2151.,\n",
       "         2152., 2153., 2154., 2155., 2156., 2157., 2158., 2159., 2160.,\n",
       "         2161., 2162., 2163., 2164., 2165., 2166., 2167., 2168., 2169.,\n",
       "         2170., 2171., 2172., 2173., 2174., 2175., 2176., 2177., 2178.,\n",
       "         2179., 2180., 2181., 2182., 2183., 2184., 2185., 2186., 2187.,\n",
       "         2188., 2189., 2190., 2191., 2192., 2193., 2194., 2195., 2196.,\n",
       "         2197., 2198., 2199., 2200., 2201., 2202., 2203., 2204., 2205.,\n",
       "         2206., 2207., 2208., 2209., 2210., 2211., 2212., 2213., 2214.,\n",
       "         2215., 2216., 2217., 2218., 2219., 2220., 2221., 2222., 2223.,\n",
       "         2224., 2225., 2226., 2227., 2228., 2229., 2230., 2231., 2232.,\n",
       "         2233., 2234., 2235., 2236., 2237., 2238., 2239., 2240., 2241.,\n",
       "         2242., 2243., 2244., 2245., 2246., 2247., 2248., 2249., 2250.,\n",
       "         2251., 2252., 2253., 2254., 2255., 2256., 2257., 2258., 2259.,\n",
       "         2260., 2261., 2262., 2263., 2264., 2265., 2266., 2267., 2268.,\n",
       "         2269., 2270., 2271., 2272., 2273., 2274., 2275., 2276., 2277.,\n",
       "         2278., 2279., 2280., 2281., 2282., 2283., 2284., 2285., 2286.,\n",
       "         2287., 2288., 2289., 2290., 2291., 2292., 2293., 2294., 2295.,\n",
       "         2296., 2297., 2298., 2299., 2300., 2301., 2302., 2303., 2304.,\n",
       "         2305., 2306., 2307., 2308., 2309., 2310., 2311., 2312., 2313.,\n",
       "         2314., 2315., 2316., 2317., 2318., 2319., 2320., 2321., 2322.,\n",
       "         2323., 2324., 2325., 2326., 2327., 2328., 2329., 2330., 2331.,\n",
       "         2332., 2333., 2334., 2335., 2336., 2337., 2338., 2339., 2340.,\n",
       "         2341., 2342., 2343., 2344., 2345., 2346., 2347., 2348., 2349.,\n",
       "         2350., 2351., 2352., 2353., 2354., 2355., 2356., 2357., 2358.,\n",
       "         2359., 2360., 2361., 2362., 2363., 2364., 2365., 2366., 2367.,\n",
       "         2368., 2369., 2370., 2371., 2372., 2373., 2374., 2375., 2376.,\n",
       "         2377., 2378., 2379., 2380., 2381., 2382., 2383., 2384., 2385.,\n",
       "         2386., 2387., 2388., 2389., 2390., 2391., 2392., 2393., 2394.,\n",
       "         2395., 2396., 2397., 2398., 2399., 2400., 2401., 2402., 2403.,\n",
       "         2404., 2405., 2406., 2407., 2408., 2409., 2410., 2411., 2412.,\n",
       "         2413., 2414., 2415., 2416., 2417., 2418., 2419., 2420., 2421.,\n",
       "         2422., 2423., 2424., 2425., 2426., 2427., 2428., 2429., 2430.,\n",
       "         2431., 2432., 2433., 2434., 2435., 2436., 2437., 2438., 2439.,\n",
       "         2440., 2441., 2442., 2443., 2444., 2445., 2446., 2447., 2448.,\n",
       "         2449., 2450., 2451., 2452., 2453., 2454., 2455., 2456., 2457.,\n",
       "         2458., 2459., 2460., 2461., 2462., 2463., 2464., 2465., 2466.,\n",
       "         2467., 2468., 2469., 2470., 2471., 2472., 2473., 2474., 2475.,\n",
       "         2476., 2477., 2478., 2479., 2480., 2481., 2482., 2483., 2484.,\n",
       "         2485., 2486., 2487., 2488., 2489., 2490., 2491., 2492., 2493.,\n",
       "         2494., 2495., 2496., 2497., 2498., 2499., 2500., 2501., 2502.,\n",
       "         2503., 2504., 2505., 2506., 2507., 2508., 2509., 2510., 2511.,\n",
       "         2512., 2513., 2514., 2515., 2516., 2517., 2518., 2519., 2520.,\n",
       "         2521., 2522., 2523., 2524., 2525., 2526., 2527., 2528., 2529.,\n",
       "         2530., 2531., 2532., 2533., 2534., 2535., 2536., 2537., 2538.,\n",
       "         2539., 2540., 2541., 2542., 2543., 2544., 2545., 2546., 2547.,\n",
       "         2548., 2549., 2550., 2551., 2552., 2553., 2554., 2555., 2556.,\n",
       "         2557., 2558., 2559., 2560., 2561., 2562., 2563., 2564., 2565.,\n",
       "         2566., 2567., 2568., 2569., 2570., 2571., 2572., 2573., 2574.,\n",
       "         2575., 2576., 2577., 2578., 2579., 2580., 2581., 2582., 2583.,\n",
       "         2584., 2585., 2586., 2587., 2588., 2589., 2590., 2591., 2592.,\n",
       "         2593., 2594., 2595., 2596., 2597., 2598., 2599., 2600., 2601.,\n",
       "         2602., 2603., 2604., 2605., 2606.]),\n",
       "  array([1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749,\n",
       "         1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760,\n",
       "         1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771,\n",
       "         1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782,\n",
       "         1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793,\n",
       "         1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804,\n",
       "         1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815,\n",
       "         1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826,\n",
       "         1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837,\n",
       "         1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848,\n",
       "         1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859,\n",
       "         1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870,\n",
       "         1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881,\n",
       "         1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892,\n",
       "         1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903,\n",
       "         1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914,\n",
       "         1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925,\n",
       "         1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936,\n",
       "         1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947,\n",
       "         1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958,\n",
       "         1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969,\n",
       "         1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980,\n",
       "         1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991,\n",
       "         1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002,\n",
       "         2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013,\n",
       "         2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024,\n",
       "         2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035,\n",
       "         2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046,\n",
       "         2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057,\n",
       "         2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068,\n",
       "         2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079,\n",
       "         2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090,\n",
       "         2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101,\n",
       "         2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112,\n",
       "         2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120])),\n",
       " (array([1739., 1740., 1741., 1742., 1743., 1744., 1745., 1746., 1747.,\n",
       "         1748., 1749., 1750., 1751., 1752., 1753., 1754., 1755., 1756.,\n",
       "         1757., 1758., 1759., 1760., 1761., 1762., 1763., 1764., 1765.,\n",
       "         1766., 1767., 1768., 1769., 1770., 1771., 1772., 1773., 1774.,\n",
       "         1775., 1776., 1777., 1778., 1779., 1780., 1781., 1782., 1783.,\n",
       "         1784., 1785., 1786., 1787., 1788., 1789., 1790., 1791., 1792.,\n",
       "         1793., 1794., 1795., 1796., 1797., 1798., 1799., 1800., 1801.,\n",
       "         1802., 1803., 1804., 1805., 1806., 1807., 1808., 1809., 1810.,\n",
       "         1811., 1812., 1813., 1814., 1815., 1816., 1817., 1818., 1819.,\n",
       "         1820., 1821., 1822., 1823., 1824., 1825., 1826., 1827., 1828.,\n",
       "         1829., 1830., 1831., 1832., 1833., 1834., 1835., 1836., 1837.,\n",
       "         1838., 1839., 1840., 1841., 1842., 1843., 1844., 1845., 1846.,\n",
       "         1847., 1848., 1849., 1850., 1851., 1852., 1853., 1854., 1855.,\n",
       "         1856., 1857., 1858., 1859., 1860., 1861., 1862., 1863., 1864.,\n",
       "         1865., 1866., 1867., 1868., 1869., 1870., 1871., 1872., 1873.,\n",
       "         1874., 1875., 1876., 1877., 1878., 1879., 1880., 1881., 1882.,\n",
       "         1883., 1884., 1885., 1886., 1887., 1888., 1889., 1890., 1891.,\n",
       "         1892., 1893., 1894., 1895., 1896., 1897., 1898., 1899., 1900.,\n",
       "         1901., 1902., 1903., 1904., 1905., 1906., 1907., 1908., 1909.,\n",
       "         1910., 1911., 1912., 1913., 1914., 1915., 1916., 1917., 1918.,\n",
       "         1919., 1920., 1921., 1922., 1923., 1924., 1925., 1926., 1927.,\n",
       "         1928., 1929., 1930., 1931., 1932., 1933., 1934., 1935., 1936.,\n",
       "         1937., 1938., 1939., 1940., 1941., 1942., 1943., 1944., 1945.,\n",
       "         1946., 1947., 1948., 1949., 1950., 1951., 1952., 1953., 1954.,\n",
       "         1955., 1956., 1957., 1958., 1959., 1960., 1961., 1962., 1963.,\n",
       "         1964., 1965., 1966., 1967., 1968., 1969., 1970., 1971., 1972.,\n",
       "         1973., 1974., 1975., 1976., 1977., 1978., 1979., 1980., 1981.,\n",
       "         1982., 1983., 1984., 1985., 1986., 1987., 1988., 1989., 1990.,\n",
       "         1991., 1992., 1993., 1994., 1995., 1996., 1997., 1998., 1999.,\n",
       "         2000., 2001., 2002., 2003., 2004., 2005., 2006., 2007., 2008.,\n",
       "         2009., 2010., 2011., 2012., 2013., 2014., 2015., 2016., 2017.,\n",
       "         2018., 2019., 2020., 2021., 2022., 2023., 2024., 2025., 2026.,\n",
       "         2027., 2028., 2029., 2030., 2031., 2032., 2033., 2034., 2035.,\n",
       "         2036., 2037., 2038., 2039., 2040., 2041., 2042., 2043., 2044.,\n",
       "         2045., 2046., 2047., 2048., 2049., 2050., 2051., 2052., 2053.,\n",
       "         2054., 2055., 2056., 2057., 2058., 2059., 2060., 2061., 2062.,\n",
       "         2063., 2064., 2065., 2066., 2067., 2068., 2069., 2070., 2071.,\n",
       "         2072., 2073., 2074., 2075., 2076., 2077., 2078., 2079., 2080.,\n",
       "         2081., 2082., 2083., 2084., 2085., 2086., 2087., 2088., 2089.,\n",
       "         2090., 2091., 2092., 2093., 2094., 2095., 2096., 2097., 2098.,\n",
       "         2099., 2100., 2101., 2102., 2103., 2104., 2105., 2106., 2107.,\n",
       "         2108., 2109., 2110., 2111., 2112., 2113., 2114., 2115., 2116.,\n",
       "         2117., 2118., 2119., 2120., 1278., 1279., 1280., 1281., 1282.,\n",
       "         1283., 1284., 1285., 1286., 1287., 1288., 1289., 1290., 1291.,\n",
       "         1292., 1293., 1294., 1295., 1296., 1297., 1298., 1299., 1300.,\n",
       "         1301., 1302., 1303., 1304., 1305., 1306., 1307., 1308., 1309.,\n",
       "         1310., 1311., 1312., 1313., 1314., 1315., 1316., 1317., 1318.,\n",
       "         1319., 1320., 1321., 1322., 1323., 1324., 1325., 1326., 1327.,\n",
       "         1328., 1329., 1330., 1331., 1332., 1333., 1334., 1335., 1336.,\n",
       "         1337., 1338., 1339., 1340., 1341., 1342., 1343., 1344., 1345.,\n",
       "         1346., 1347., 1348., 1349., 1350., 1351., 1352., 1353., 1354.,\n",
       "         1355., 1356., 1357., 1358., 1359., 1360., 1361., 1362., 1363.,\n",
       "         1364., 1365., 1366., 1367., 1368., 1369., 1370., 1371., 1372.,\n",
       "         1373., 1374., 1375., 1376., 1377., 1378., 1379., 1380., 1381.,\n",
       "         1382., 1383., 1384., 1385., 1386., 1387., 1388., 1389., 1390.,\n",
       "         1391., 1392., 1393., 1394., 1395., 1396., 1397., 1398., 1399.,\n",
       "         1400., 1401., 1402., 1403., 1404., 1405., 1406., 1407., 1408.,\n",
       "         1409., 1410., 1411., 1412., 1413., 1414., 1415., 1416., 1417.,\n",
       "         1418., 1419., 1420., 1421., 1422., 1423., 1424., 1425., 1426.,\n",
       "         1427., 1428., 1429., 1430., 1431., 1432., 1433., 1434., 1435.,\n",
       "         1436., 1437., 1438., 1439., 1440., 1441., 1442., 1443., 1444.,\n",
       "         1445., 1446., 1447., 1448., 1449., 1450., 1451., 1452., 1453.,\n",
       "         1454., 1455., 1456., 1457., 1458., 1459., 1460., 1461., 1462.,\n",
       "         1463., 1464., 1465., 1466., 1467., 1468., 1469., 1470., 1471.,\n",
       "         1472., 1473., 1474., 1475., 1476., 1477., 1478., 1479., 1480.,\n",
       "         1481., 1482., 1483., 1484., 1485., 1486., 1487., 1488., 1489.,\n",
       "         1490., 1491., 1492., 1493., 1494., 1495., 1496., 1497., 1498.,\n",
       "         1499., 1500., 1501., 1502., 1503., 1504., 1505., 1506., 1507.,\n",
       "         1508., 1509., 1510., 1511., 1512., 1513., 1514., 1515., 1516.,\n",
       "         1517., 1518., 1519., 1520., 1521., 1522., 1523., 1524., 1525.,\n",
       "         1526., 1527., 1528., 1529., 1530., 1531., 1532., 1533., 1534.,\n",
       "         1535., 1536., 1537., 1538., 1539., 1540., 1541., 1542., 1543.,\n",
       "         1544., 1545., 1546., 1547., 1548., 1549., 1550., 1551., 1552.,\n",
       "         1553., 1554., 1555., 1556., 1557., 1558., 1559., 1560., 1561.,\n",
       "         1562., 1563., 1564., 1565., 1566., 1567., 1568., 1569., 1570.,\n",
       "         1571., 1572., 1573., 1574., 1575., 1576., 1577., 1578., 1579.,\n",
       "         1580., 1581., 1582., 1583., 1584., 1585., 1586., 1587., 1588.,\n",
       "         1589., 1590., 1591., 1592., 1593., 1594., 1595., 1596., 1597.,\n",
       "         1598., 1599., 1600., 1601., 1602., 1603., 1604., 1605., 1606.,\n",
       "         1607., 1608., 1609., 1610., 1611., 1612., 1613., 1614., 1615.,\n",
       "         1616., 1617., 1618., 1619., 1620., 1621., 1622., 1623., 1624.,\n",
       "         1625., 1626., 1627., 1628., 1629., 1630., 1631., 1632., 1633.,\n",
       "         1634., 1635., 1636., 1637., 1638., 1639., 1640., 1641., 1642.,\n",
       "         1643., 1644., 1645., 1646., 1647., 1648., 1649., 1650., 1651.,\n",
       "         1652., 1653., 1654., 1655., 1656., 1657., 1658., 1659., 1660.,\n",
       "         1661., 1662., 1663., 1664., 1665., 1666., 1667., 1668., 1669.,\n",
       "         1670., 1671., 1672., 1673., 1674., 1675., 1676., 1677., 1678.,\n",
       "         1679., 1680., 1681., 1682., 1683., 1684., 1685., 1686., 1687.,\n",
       "         1688., 1689., 1690., 1691., 1692., 1693., 1694., 1695., 1696.,\n",
       "         1697., 1698., 1699., 1700., 1701., 1702., 1703., 1704., 1705.,\n",
       "         1706., 1707., 1708., 1709., 1710., 1711., 1712., 1713., 1714.,\n",
       "         1715., 1716., 1717., 1718., 1719., 1720., 1721., 1722., 1723.,\n",
       "         1724., 1725., 1726., 1727., 1728., 1729., 1730., 1731., 1732.,\n",
       "         1733., 1734., 1735., 1736., 1737., 1738.]),\n",
       "  array([ 852,  853,  854,  855,  856,  857,  858,  859,  860,  861,  862,\n",
       "          863,  864,  865,  866,  867,  868,  869,  870,  871,  872,  873,\n",
       "          874,  875,  876,  877,  878,  879,  880,  881,  882,  883,  884,\n",
       "          885,  886,  887,  888,  889,  890,  891,  892,  893,  894,  895,\n",
       "          896,  897,  898,  899,  900,  901,  902,  903,  904,  905,  906,\n",
       "          907,  908,  909,  910,  911,  912,  913,  914,  915,  916,  917,\n",
       "          918,  919,  920,  921,  922,  923,  924,  925,  926,  927,  928,\n",
       "          929,  930,  931,  932,  933,  934,  935,  936,  937,  938,  939,\n",
       "          940,  941,  942,  943,  944,  945,  946,  947,  948,  949,  950,\n",
       "          951,  952,  953,  954,  955,  956,  957,  958,  959,  960,  961,\n",
       "          962,  963,  964,  965,  966,  967,  968,  969,  970,  971,  972,\n",
       "          973,  974,  975,  976,  977,  978,  979,  980,  981,  982,  983,\n",
       "          984,  985,  986,  987,  988,  989,  990,  991,  992,  993,  994,\n",
       "          995,  996,  997,  998,  999, 1000, 1001, 1002, 1003, 1004, 1005,\n",
       "         1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016,\n",
       "         1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027,\n",
       "         1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038,\n",
       "         1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049,\n",
       "         1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060,\n",
       "         1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071,\n",
       "         1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082,\n",
       "         1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093,\n",
       "         1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104,\n",
       "         1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115,\n",
       "         1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126,\n",
       "         1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137,\n",
       "         1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148,\n",
       "         1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159,\n",
       "         1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170,\n",
       "         1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181,\n",
       "         1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192,\n",
       "         1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203,\n",
       "         1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214,\n",
       "         1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225,\n",
       "         1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236,\n",
       "         1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247,\n",
       "         1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258,\n",
       "         1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269,\n",
       "         1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277]))]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating masks for gridsearch cv:\n",
    "\n",
    "#create vertical cv_splits\n",
    "myCViterator = []\n",
    "for f,fold in enumerate(pianist_cv):\n",
    "    vetrical_mask_train=np.zeros(0)\n",
    "    vetrical_mask_test=np.zeros(0)\n",
    "    for i in range(len(fold)):\n",
    "        mask= mask#X_tags[X_tags['Pianist']!=pianist_cv[f][i]].index.values.astype(int)\n",
    "        vetrical_mask_train=np.concatenate((vetrical_mask_train,mask), axis=0)\n",
    "        \n",
    "        mask= X_tags[X_tags['Pianist']==pianist_cv[f][i]].index.values.astype(int)\n",
    "        vetrical_mask_test=mask#np.concatenate((vetrical_mask_test,mask), axis=0)\n",
    "    #print(vetrical_mask_test)\n",
    "    trainIndices=vetrical_mask_train\n",
    "    testIndices=vetrical_mask_test\n",
    "    myCViterator.append((trainIndices, testIndices))\n",
    "\n",
    "    \n",
    "'''\n",
    "for i in range(nFolds):\n",
    "    trainIndices = myDf[ myDf['cvLabel']!=i ].index.values.astype(int)\n",
    "    testIndices =  myDf[ myDf['cvLabel']==i ].index.values.astype(int)\n",
    "    myCViterator.append( (trainIndices, testIndices) )\n",
    "vetrical_mask=[[2,3,4],[3,4,54]]\n",
    "'''\n",
    "myCViterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_determine_key_type\u001b[1;34m(key, accept_slice)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0marray_dtype_to_str\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'f'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-3a8fbe20bf93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgs_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMLPClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_layer_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmyCViterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgs_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    708\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 710\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1149\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    680\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    683\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1002\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    256\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    256\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[1;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    388\u001b[0m         )\n\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m     \u001b[0mindices_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_determine_key_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mindices_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'str'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_determine_key_type\u001b[1;34m(key, accept_slice)\u001b[0m\n\u001b[0;32m    285\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0marray_dtype_to_str\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed"
     ]
    }
   ],
   "source": [
    "gs_cv = GridSearchCV(MLPClassifier(hidden_layer_sizes=(4,40)), param_grid = {}, cv=myCViterator)\n",
    "gs_cv.fit(pd.DataFrame(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.03590298]),\n",
       " 'std_fit_time': array([0.00498939]),\n",
       " 'mean_score_time': array([0.00448489]),\n",
       " 'std_score_time': array([0.00050163]),\n",
       " 'params': [{}],\n",
       " 'split0_test_score': array([1.]),\n",
       " 'split1_test_score': array([1.]),\n",
       " 'mean_test_score': array([1.]),\n",
       " 'std_test_score': array([0.]),\n",
       " 'rank_test_score': array([1])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cv.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers_structur=(4,40) #(4,40) seems to be the optimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by pianist cross validation\n",
    "score=0\n",
    "for pianist in pianist_list:\n",
    "    mask1=X_tags['Pianist']!=pianist\n",
    "    mask2=X_tags['Pianist']==pianist\n",
    "    X_train, y_train=X[mask1],y[mask1]\n",
    "    X_test, y_test=X[mask2],y[mask2]\n",
    "    #print(X_train)\n",
    "    clf = MLPClassifier(random_state=1, hidden_layer_sizes=(4,40),max_iter=400).fit(X_train, y_train)\n",
    "    score+=clf.score(X_test, y_test)\n",
    "    #print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5580338008076287\n"
     ]
    }
   ],
   "source": [
    "pianist_cross_validation_score=score/len(pianist_list)\n",
    "print(pianist_cross_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by piece cross validation\n",
    "#split dataset into train and test data\n",
    "score=0\n",
    "for piece in piece_list:\n",
    "    mask1=X_tags['Piece_id']!=piece\n",
    "    mask2=X_tags['Piece_id']==piece\n",
    "    #print(len(mask1))\n",
    "    X_train, y_train=X[mask1],y[mask1]\n",
    "    X_test, y_test=X[mask2],y[mask2]\n",
    "    clf = MLPClassifier(random_state=1, hidden_layer_sizes=(5,50), max_iter=300).fit(X_train, y_train)\n",
    "    score+=clf.score(X_test, y_test)# Wrong!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    #xou need to do test size correction! e.g score+=score*length(test set)\n",
    "    # and finally score /all test set sizes together\n",
    "    \n",
    "    #print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3626491402617777\n"
     ]
    }
   ],
   "source": [
    "piece_cross_validation_score=score/len(piece_list)\n",
    "print(piece_cross_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent done: 4.62962962962963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent done: 9.25925925925926\n",
      "percent done: 13.88888888888889\n",
      "percent done: 18.51851851851852\n",
      "percent done: 23.14814814814815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent done: 27.77777777777778\n",
      "percent done: 32.407407407407405\n",
      "percent done: 37.03703703703704\n",
      "percent done: 41.66666666666667\n",
      "percent done: 46.2962962962963\n",
      "percent done: 50.92592592592593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent done: 55.55555555555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent done: 60.18518518518518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent done: 64.81481481481481\n",
      "percent done: 69.44444444444444\n",
      "percent done: 74.07407407407408\n",
      "percent done: 78.70370370370371\n",
      "percent done: 83.33333333333334\n",
      "percent done: 87.96296296296296\n",
      "percent done: 92.5925925925926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\AI\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:568: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent done: 97.22222222222221\n"
     ]
    }
   ],
   "source": [
    "#by piece and pianist cross validation\n",
    "#split dataset into train and test data\n",
    "score=0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "c=0\n",
    "for i, piece in enumerate(piece_list):\n",
    "    for j, pianist in enumerate(pianist_list):\n",
    "        c+=1\n",
    "        # we can not do ((X_tags['Piece_id'] ==piece) & (X_tags['Pianist']==pianist)) and !((X_tags['Piece_id'] ==piece) & (X_tags['Pianist']==pianist))\n",
    "        # because the we would have unwanted correaltions between \n",
    "        #((X_tags['Piece_id'] ==piece) & (X_tags['Pianist']==pianist))\n",
    "        #and ((X_tags['Piece_id'] !=piece) & (X_tags['Pianist']==pianist))\n",
    "        mask2 = ((X_tags['Piece_id'] ==piece) & (X_tags['Pianist']==pianist))\n",
    "        mask1 = ((X_tags['Piece_id'] !=piece) & (X_tags['Pianist']!=pianist))\n",
    "                         \n",
    "        X_train, y_train=X[mask1],y[mask1]\n",
    "        X_test, y_test=X[mask2],y[mask2]\n",
    "        #print(X_train)\n",
    "        clf = MLPClassifier(random_state=1,hidden_layer_sizes=hidden_layers_structur, max_iter=300).fit(X_train, y_train)\n",
    "        score+=clf.score(X_test, y_test)\n",
    "        if c%10==0:\n",
    "            print(f'percent done: {(c)/(len(pianist_list)*len(piece_list))*100}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3455958986626032\n"
     ]
    }
   ],
   "source": [
    "both_cross_validation_score=score/(len(pianist_list)*len(piece_list))\n",
    "print(both_cross_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the score is slightly lower, but one must keep in mind,\n",
    "# that in order to eliminate all correlations between test and training set\n",
    "# we dropped (!pianist and piece) and (pianist and !piece) \n",
    "# in other words our training set size was quite a bit smaller!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "ai"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
